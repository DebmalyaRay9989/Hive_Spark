Spark Sql

spark2-shell

Take the data from hdfs

var myRdd=sc.textFile("/user/edureka_639001/emp.txt")

myRdd.collect

myRdd.map(x=>x.split(","));

case class EmpClass(empId:Int, name:String,age:Int,salary:Float,dept:String);

myRdd.map(x=>x.split(",")).map(x=>EmpClass(x(0).toInt,x(1),x(2).toInt,x(3).toFloat,x(4)));

var cRdd=myRdd.map(x=>x.split(",")).map(x=>EmpClass(x(0).toInt,x(1),x(2).toInt,x(3).toFloat,x(4)));

cRdd.toDF

var myDF=cRdd.toDF

myDF.show
+-----+---------+---+------+-------+
|empId|     name|age|salary|   dept|
+-----+---------+---+------+-------+
|    1|  Sheldon| 30|5000.0|     IT|
|    2|  Leonard| 29|3500.0|     HR|
|    3|    Penny| 29|2300.0|     IT|
|    4|      Raj| 32|3200.0|     HR|
|    5|   Howard| 31|4300.0|Finance|
|    6|Bernadate| 29|3500.0|     IT|
|    7|      Amy| 30|3210.0|Finance|
+-----+---------+---+------+-------+


myDF.printSchema
root
 |-- empId: integer (nullable = true)
 |-- name: string (nullable = true)
 |-- age: integer (nullable = true)
 |-- salary: float (nullable = true)
 |-- dept: string (nullable = true)


myDF.select("name","age").show

https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/DataFrame.html

 myDF.select("name","age").show
+---------+---+
|     name|age|
+---------+---+
|  Sheldon| 30|
|  Leonard| 29|
|    Penny| 29|
|      Raj| 32|
|   Howard| 31|
|Bernadate| 29|
|      Amy| 30|
+---------+---+

myDF.filter("dept='IT'").show

myDF.select("name","age","salary").filter("dept='IT'").show

myDF.select("name","age","salary").filter("dept='IT'and salary>3500").show

myDF.select($"name",$"age",$"salary").where("dept='IT'and salary>3500").show

where is as good as filter

//Sorting

myDF.sort("dept").show

myDF.orderBy(col("dept")).show

myDF.orderBy(col("dept").desc).show

myDF.orderBy(col("dept").desc,col("salary")).show

--Expression

myDF.select(col("name"),expr("salary * 1.1")).show

myDF.select(col("name"),expr("salary * 1.25") as "Revised Salary").show

myDF.select(expr("name"),expr("salary +1500"),expr("name like 'P%'")).show

Loading file through session

spark.read.format("csv").option("delimiter",",").option("header","false").load("/user/edureka_639001/emp.txt")

var myDF=spark.read.format("csv").option("delimiter",",").option("header","false").load("/user/edureka_639001/emp.txt")

myDF.printSchema
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)


var myDF=spark.read.format("csv").option("delimiter",",").option("header","false").option("inferSchema","true").load("/user/edureka_639001/emp.txt").toDF(column Names here)



myDF.select(col("_c0").cast(org.apache.spark.sql.types.IntegerType))

myDF.toDF("empID","name","age","salary","dept")



groupBy-----------

myDF.groupBy("dept").agg(count(col("empID")),sum(col("salary"))).show

myDF.groupBy("dept","age").agg(count(col("empID")),sum(col("salary"))).show

myDF.groupBy("dept").agg(Map("salary"->"sum","empID"->"count")).show

Loading Data as DataSet

case class Cricket(name:String,position:String)
var c=spark.read.json("/user/edureka_918210/Deb_DataSets/people.json").as[People]



Changing Log Level

sc.setLogLevel("info")

sc.setLogLevel("error")


Text Analysis - Sentiment Analysis

var ia=Array("Zarab lives in Mumbai","Tom  is an expert of big data")

var df=sc.parallelize(ia).toDF("value")

df.show(false)

df.explode("value","words"){x:String => x.split(" ")}.show(false)

or

df.withColumn("words",explode(split(col("value")," "))).show(false)

Loading JSON DataFrame

var data=Array("""{"Name":"Alex","Age":"25","CoWorkers":["Mike","John"],"Address":{"city":"Mumbai","state":"MH","country":"India","pincode":"400004"}}""",
"""{"Name":"Mike","Age":"30","CoWorkers":["Alex","John"],"Address":{"city":"Bengaluru","state":"KA","country":"India"}}"""
)


https://www.learningcontainer.com/sample-json-file/

val data = spark.read.json("/user/edureka_918210/Deb_DataSets/people.json")
var df=sc.parallelize(data).toDF("value")

df.show(false)

df.withColumn("name",get_json_object(col("value"),"$.Name")).show

df.withColumn("name",get_json_object(col("value"),"$.Name")).withColumn("city",get_json_object(col("value"),"$.Address.city")).show(false)

df.withColumn("name",get_json_object(col("value"),"$.Name")).withColumn("city",get_json_object(col("value"),"$.Address.pincode")).show

df.withColumn("name",get_json_object(col("value"),"$.Name")).withColumn("city",get_json_object(col("value"),"$.Address.pincode")).drop("value").show



Reading Data from a json file
spark.read.json("/user/edureka_639001/employee.json")
data.printSchema
data.createOrReplaceTempView("employee")
var empData=spark.sql("Select name from employee");
empData.show

//SEt Operations

var set1DF=spark.read.format("csv").option("delimiter",",").option("header","true").load("/user/edureka_639001/set1.txt")

var set2DF=spark.read.format("csv").option("delimiter",",").option("header","true").load("/user/edureka_639001/set2.txt")

set1DF.union(set2DF).show

set1DF.intersect(set2DF).show

set1DF.except(set2DF).show

set2DF.except(set1DF).show

set1DF.select("id").intersect(set2DF.select("id")).show

var unionDF=set1DF.union(set2DF)
unionDF.groupBy("id").agg(count(col("id")).as("count")).filter("count>1").show

//Joins

var players=spark.read.format("csv").option("delimiter",",").option("header","true").load("/user/edureka_639001/players.txt")

var records=spark.read.format("csv").option("delimiter",",").option("header","true").load("/user/edureka_639001/records.txt")

players.join(records,players.col("id")===records.col("pid")).show(false)

players.join(records,players.col("id")===records.col("pid"),"full_outer").show

players.join(records,players.col("id")===records.col("pid"),"full_outer").select("name","country","record").withColumn("record",when(col("record").isNull,lit("No record"))).show


players.createOrReplaceTempView("players")
records.createOrReplaceTempView("records")
 spark.sql("Select p.name,p.country,r.record from players p join records r on p.id=r.pid").show

 spark.sql("Select p.name,p.country,r.record from players p join records r on p.id=r.pid where p.country='India'").show

 spark.sql("Select p.name,p.country,r.record from players p left outer join records r on p.id=r.pid where p.country='India'").show

 spark.sql("Select p.country,count(r.record) from players p join records r on p.id=r.pid  group by p.country").show

-- Analytical Functions

var myDF=spark.read.format("csv").option("delimiter",",").option("header","false").option("inferSchema","true").load("/user/edureka_639001/emp.txt").toDF("empID","name","age","salary","dept")

 import org.apache.spark.sql.expressions

 var par= expressions.Window.partitionBy(col("dept"))
 https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/expressions/Window.html

myDF.withColumn("rank",row_number() over par.orderBy(col("salary").desc)).sort("dept").filter("rank <=2").show

The row_number() is a window function in Spark SQL that assigns a row number (sequential integer number) 
to each row in the result DataFrame. This function is used with Window.partitionBy() which
 partitions the data into windows frames and orderBy() clause to sort the rows in each partition.

myDF.createOrReplaceTempView("employee")

spark.sql("Select name,age,salary,dept,row_number() over (partition by dept order by salary desc) rank  from employee ").show

spark.sql("Select name,age,salary,dept,row_number() over (partition by dept order by salary desc) rank  from employee ").show

spark.sql("Select name,age,salary,dept,row_number() over (partition by dept order by salary desc) rank,max(salary) over(partition by dept) maxsal  from employee order by dept ").show

 ---Storing the data in hdfs

  spark.sql("Select name,age,salary,dept,row_number() over (partition by dept order by salary desc) rank,max(salary) over(partition by
 dept) maxsal  from employee order by dept ").write.format("csv").option("delimiter",",").option("header","true").save("/user/edureka_63900
1/empDataInHDFS.csv")

-- Loading Data from OC and Parquet format
spark.sql("Select name,age,salary,dept,row_number() over (partition by dept order by salary desc) rank,max(salary) over(partition by dept) maxsal  from employee order by dept ").write.format("orc").save("/user/edureka_639001/mydata.orc")

 var xyz=spark.read.format("orc").load("/user/edureka_639001/mydata.orc")

 Storing using dept partition
 myDF.write.partitionBy("dept").format("json").save("/user/edureka_639001/sparkdata")


var x=spark.read.json("/user/edureka_639001/sparkdata/dept=IT")

-- Connecting Hive table to spark
var z=spark.read.table("edureka_247.txn_ext")














